Corpus composition
------------------

Collecting all tweets generated by all lawmakers from congresses 115 &
116 (3 January 2017 to present day) in one fell swoop is not possible
via the Twitter API. Instead, the best way to build such a corpus would
have been to scrape tweets daily or weekly for congress members starting
at 3 January 2017.

While this ship has sailed for me (at least for the 115th and most of
the 116th), other/smarter folks have been doing this – eg – [the George
Washington University (GWU)
Library](https://tweetsets.library.gwu.edu/). GWU makes available Tweet
IDs for this set of tweets, and [DocNow’s
Hydrator](https://github.com/DocNow/hydrator), eg, can be used to obtain
actual tweet data based on the Tweet ID.

I then keep the (116th portion of the) corpus up-to-date using the R
package `rtweet`, collecting status updates for Twitter handles used to
build the GWU tweet collection. Via an automated daily script that can
be used in subsequent congresses. From this perspective, then, we only
use the GWU tweet sets to get caught up on the congressional tweet
collection.

### Descriptives
